{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " All required import statement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import tensorflow as tf\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback\n",
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "\n",
    "sns.set_theme(style='white', palette='viridis')\n",
    "pal = sns.color_palette('viridis')\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "set_config(transform_output='pandas')\n",
    "pd.options.mode.chained_assignment = None\n",
    "seed = 42\n",
    "tss = TimeSeriesSplit(10)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '42'"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "c:\\Users\\jinqw\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " This section will prepare the cross_validation function\n",
    " cv = time series split of 10"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def cross_validation(estimator, save_path, X, y, X_test, cv=tss, label=''):\n",
    "    \"\"\"cross validation function\n",
    "\n",
    "    Args:\n",
    "        estimator (model): chosen model \n",
    "        save_path (str) : target directory to save the model\n",
    "        X (dataframe): Independent features for training\n",
    "        y (dataframe): dependent features for training  \n",
    "        X_test (dataframe): Independent features for testing\n",
    "        cv (split, optional): split for the cross validation. Defaults to tss.\n",
    "        label (str, optional): special label. Defaults to ''.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    save_path = 'models_raw'\n",
    "    # Build the save path if not exist\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # initiate prediction arrays and score lists\n",
    "    val_predictions = np.zeros((len(X)))\n",
    "    # train_predictions = np.zeros((len(sample)))\n",
    "    train_scores, val_scores = [], []\n",
    "    best_model = None\n",
    "    best_model_train_score = 0\n",
    "    best_val_score = 0\n",
    "\n",
    "    # training model, predicting prognosis probability, and evaluating metrics\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "\n",
    "        model = clone(estimator)\n",
    "\n",
    "        # define train set\n",
    "        X_train = X.iloc[train_idx]\n",
    "        y_train = y.iloc[train_idx]\n",
    "\n",
    "        # define validation set\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        # train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Save the model\n",
    "        joblib.dump(model, f'./{save_path}/{label}_{fold}.model')\n",
    "\n",
    "        # make predictions\n",
    "        train_preds = model.predict(X_train)\n",
    "        val_preds = model.predict(X_val)\n",
    "\n",
    "        val_predictions[val_idx] += val_preds\n",
    "\n",
    "        # evaluate model for a fold\n",
    "        train_score = mean_absolute_error(y_train, train_preds)\n",
    "        val_score = mean_absolute_error(y_val, val_preds)\n",
    "\n",
    "        # Update best model\n",
    "        if best_val_score == 0 or val_score < best_val_score:\n",
    "            best_val_score = val_score\n",
    "            best_model_train_score = train_score\n",
    "            best_model = model\n",
    "\n",
    "        # append model score for a fold to list\n",
    "        train_scores.append(train_score)\n",
    "        val_scores.append(val_score)\n",
    "\n",
    "\n",
    "    # This line print the average\n",
    "    print(f'Val Score: {np.mean(val_scores):.5f} ± {np.std(val_scores):.5f} | Train Score: {np.mean(train_scores):.5f} ± {np.std(train_scores):.5f} | {label}')\n",
    "    # Print best model score\n",
    "    print(\n",
    "        f'Best validation score: {best_val_score}, associated train score: {best_model_train_score}')\n",
    "    joblib.dump(best_model, f'./{save_path}/best_model.model')\n",
    "\n",
    "    return val_scores, val_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " Prepare to read the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dtypes = {\n",
    "    'stock_id': np.uint8,\n",
    "    'date_id': np.uint16,\n",
    "    'seconds_in_bucket': np.uint16,\n",
    "    'imbalance_buy_sell_flag': np.int8,\n",
    "    'time_id': np.uint16,\n",
    "}\n",
    "\n",
    "\n",
    "def read_data(data_path: str):\n",
    "    \"\"\"Read the data from the train and test csv files, split them into the x (features) and y(target)\n",
    "\n",
    "    Args:\n",
    "        data_path (str): absolute save path for the train and test data set \n",
    "\n",
    "    Returns:\n",
    "        X (dataframe): Independent features for training\n",
    "        y (dataframe): dependent features for training  \n",
    "        X_test (dataframe): Independent features for testing\n",
    "    \"\"\"\n",
    "    # Load data from the save path\n",
    "    train = pd.read_csv(f'{data_path}/train.csv',\n",
    "                        dtype=dtypes).drop(['row_id', 'time_id'], axis=1)\n",
    "    test = pd.read_csv(f'{data_path}/test.csv',\n",
    "                       dtype=dtypes).drop(['row_id', 'time_id'], axis=1)\n",
    "\n",
    "    # Check the data set\n",
    "    train.info()\n",
    "    print(train.head())\n",
    "    gc.collect()\n",
    "\n",
    "    # split data into X and y\n",
    "    X = train[~train.target.isna()]\n",
    "    y = X.pop('target')\n",
    "    \n",
    "    # Test data dont have target column\n",
    "    X_test = test[~train.target.isna()]\n",
    "\n",
    "    X.info()\n",
    "    print(X.head())\n",
    "    return X, y, X_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " Call the required function and run the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "models = [\n",
    "    ('LightGBM', LGBMRegressor(random_state=seed, objective='mse', device_type='gpu'))\n",
    "]\n",
    "X, y, X_test = read_data(\n",
    "    \"D:/OneDrive/NEU/CS6140/optiver-trading-at-the-close\")\n",
    "model_save_path = \"initial_run\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5237980 entries, 0 to 5237979\n",
      "Data columns (total 15 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   stock_id                 uint8  \n",
      " 1   date_id                  uint16 \n",
      " 2   seconds_in_bucket        uint16 \n",
      " 3   imbalance_size           float64\n",
      " 4   imbalance_buy_sell_flag  int8   \n",
      " 5   reference_price          float64\n",
      " 6   matched_size             float64\n",
      " 7   far_price                float64\n",
      " 8   near_price               float64\n",
      " 9   bid_price                float64\n",
      " 10  bid_size                 float64\n",
      " 11  ask_price                float64\n",
      " 12  ask_size                 float64\n",
      " 13  wap                      float64\n",
      " 14  target                   float64\n",
      "dtypes: float64(11), int8(1), uint16(2), uint8(1)\n",
      "memory usage: 469.6 MB\n",
      "   stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
      "0         0        0                  0      3180602.69   \n",
      "1         1        0                  0       166603.91   \n",
      "2         2        0                  0       302879.87   \n",
      "3         3        0                  0     11917682.27   \n",
      "4         4        0                  0       447549.96   \n",
      "\n",
      "   imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
      "0                        1         0.999812   13380276.64        NaN   \n",
      "1                       -1         0.999896    1642214.25        NaN   \n",
      "2                       -1         0.999561    1819368.03        NaN   \n",
      "3                       -1         1.000171   18389745.62        NaN   \n",
      "4                       -1         0.999532   17860614.95        NaN   \n",
      "\n",
      "   near_price  bid_price  bid_size  ask_price   ask_size  wap    target  \n",
      "0         NaN   0.999812  60651.50   1.000026    8493.03  1.0 -3.029704  \n",
      "1         NaN   0.999896   3233.04   1.000660   20605.09  1.0 -5.519986  \n",
      "2         NaN   0.999403  37956.00   1.000298   18995.00  1.0 -8.389950  \n",
      "3         NaN   0.999999   2324.90   1.000214  479032.40  1.0 -4.010200  \n",
      "4         NaN   0.999394  16485.54   1.000016     434.10  1.0 -7.349849  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5237892 entries, 0 to 5237979\n",
      "Data columns (total 14 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   stock_id                 uint8  \n",
      " 1   date_id                  uint16 \n",
      " 2   seconds_in_bucket        uint16 \n",
      " 3   imbalance_size           float64\n",
      " 4   imbalance_buy_sell_flag  int8   \n",
      " 5   reference_price          float64\n",
      " 6   matched_size             float64\n",
      " 7   far_price                float64\n",
      " 8   near_price               float64\n",
      " 9   bid_price                float64\n",
      " 10  bid_size                 float64\n",
      " 11  ask_price                float64\n",
      " 12  ask_size                 float64\n",
      " 13  wap                      float64\n",
      "dtypes: float64(10), int8(1), uint16(2), uint8(1)\n",
      "memory usage: 469.6 MB\n",
      "   stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
      "0         0        0                  0      3180602.69   \n",
      "1         1        0                  0       166603.91   \n",
      "2         2        0                  0       302879.87   \n",
      "3         3        0                  0     11917682.27   \n",
      "4         4        0                  0       447549.96   \n",
      "\n",
      "   imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
      "0                        1         0.999812   13380276.64        NaN   \n",
      "1                       -1         0.999896    1642214.25        NaN   \n",
      "2                       -1         0.999561    1819368.03        NaN   \n",
      "3                       -1         1.000171   18389745.62        NaN   \n",
      "4                       -1         0.999532   17860614.95        NaN   \n",
      "\n",
      "   near_price  bid_price  bid_size  ask_price   ask_size  wap  \n",
      "0         NaN   0.999812  60651.50   1.000026    8493.03  1.0  \n",
      "1         NaN   0.999896   3233.04   1.000660   20605.09  1.0  \n",
      "2         NaN   0.999403  37956.00   1.000298   18995.00  1.0  \n",
      "3         NaN   0.999999   2324.90   1.000214  479032.40  1.0  \n",
      "4         NaN   0.999394  16485.54   1.000016     434.10  1.0  \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-3-af88a0db35c2>:37: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  X_test = test[~train.target.isna()]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for (label, model) in models:\n",
    "    _ = cross_validation(\n",
    "        make_pipeline(\n",
    "            model\n",
    "        ),\n",
    "        save_path=model_save_path,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        X_test=X_test,\n",
    "        label=label\n",
    "    )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 2848\n",
      "[LightGBM] [Info] Number of data points in the train set: 476172, number of used features: 14\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 14 dense feature groups (7.27 MB) transferred to GPU in 0.011811 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.096914\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 2895\n",
      "[LightGBM] [Info] Number of data points in the train set: 952344, number of used features: 14\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 14 dense feature groups (14.53 MB) transferred to GPU in 0.014440 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.068442\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 2939\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428516, number of used features: 14\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 14 dense feature groups (21.80 MB) transferred to GPU in 0.021936 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.063714\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 2983\n",
      "[LightGBM] [Info] Number of data points in the train set: 1904688, number of used features: 14\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 14 dense feature groups (29.06 MB) transferred to GPU in 0.025646 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.060067\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3028\n",
      "[LightGBM] [Info] Number of data points in the train set: 2380860, number of used features: 14\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 14 dense feature groups (36.33 MB) transferred to GPU in 0.033332 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.062212\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3051\n",
      "[LightGBM] [Info] Number of data points in the train set: 2857032, number of used features: 14\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 14 dense feature groups (43.59 MB) transferred to GPU in 0.037870 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.053736\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3054\n",
      "[LightGBM] [Info] Number of data points in the train set: 3333204, number of used features: 14\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 14 dense feature groups (50.86 MB) transferred to GPU in 0.044500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.042476\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3052\n",
      "[LightGBM] [Info] Number of data points in the train set: 3809376, number of used features: 14\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 14 dense feature groups (58.13 MB) transferred to GPU in 0.052408 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.043773\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3054\n",
      "[LightGBM] [Info] Number of data points in the train set: 4285548, number of used features: 14\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 14 dense feature groups (65.39 MB) transferred to GPU in 0.062428 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.049858\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 3055\n",
      "[LightGBM] [Info] Number of data points in the train set: 4761720, number of used features: 14\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 14 dense feature groups (72.66 MB) transferred to GPU in 0.073401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.043377\n",
      "Val Score: 6.47658 ± 0.54019 | Train Score: 6.14128 ± 0.42522 | LightGBM\n",
      "Best validation score: 5.885231647954695, associated train score: 6.342175608895237\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}