{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " %%[markdown]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import tensorflow as tf\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from sklearn import set_config\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "\n",
    "sns.set_theme(style = 'white', palette = 'viridis')\n",
    "pal = sns.color_palette('viridis')\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "set_config(transform_output = 'pandas')\n",
    "pd.options.mode.chained_assignment = None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " %%[markdown]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dtypes = {\n",
    "    'stock_id' : np.uint8,\n",
    "    'date_id' : np.uint16,\n",
    "    'seconds_in_bucket' : np.uint16,\n",
    "    'imbalance_buy_sell_flag' : np.int8,\n",
    "    'time_id' : np.uint16,\n",
    "}\n",
    "\n",
    "train = pd.read_csv('D:/OneDrive/NEU/CS6140/optiver-trading-at-the-close/train.csv', dtype = dtypes).drop(['row_id', 'time_id'], axis = 1)\n",
    "test = pd.read_csv('D:/OneDrive/NEU/CS6140/optiver-trading-at-the-close/test.csv', dtype = dtypes).drop(['row_id', 'time_id'], axis = 1)\n",
    "\n",
    "gc.collect()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " %%[markdown]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "X = train[~train.target.isna()]\n",
    "y = X.pop('target')\n",
    "\n",
    "seed = 42\n",
    "tss = TimeSeriesSplit(10)\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '42'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " %%[markdown]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def imbalance_calculator(x):\n",
    "    \n",
    "    features = ['seconds_in_bucket', 'imbalance_buy_sell_flag',\n",
    "               'imbalance_size', 'matched_size', 'bid_size', 'ask_size',\n",
    "                'reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap',\n",
    "                'imb_s1', 'imb_s2'\n",
    "               ]\n",
    "    \n",
    "    x_copy = x.copy()\n",
    "    \n",
    "    x_copy['imb_s1'] = x.eval('(bid_size - ask_size) / (bid_size + ask_size)')\n",
    "    x_copy['imb_s2'] = x.eval('(imbalance_size - matched_size) / (matched_size + imbalance_size)')\n",
    "    \n",
    "    prices = ['reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n",
    "    \n",
    "    for i,a in enumerate(prices):\n",
    "        for j,b in enumerate(prices):\n",
    "            if i>j:\n",
    "                x_copy[f'{a}_{b}_imb'] = x.eval(f'({a} - {b}) / ({a} + {b})')\n",
    "                features.append(f'{a}_{b}_imb')\n",
    "                    \n",
    "    for i,a in enumerate(prices):\n",
    "        for j,b in enumerate(prices):\n",
    "            for k,c in enumerate(prices):\n",
    "                if i>j and j>k:\n",
    "                    max_ = x[[a,b,c]].max(axis=1)\n",
    "                    min_ = x[[a,b,c]].min(axis=1)\n",
    "                    mid_ = x[[a,b,c]].sum(axis=1)-min_-max_\n",
    "\n",
    "                    x_copy[f'{a}_{b}_{c}_imb2'] = (max_-mid_)/(mid_-min_)\n",
    "                    features.append(f'{a}_{b}_{c}_imb2')\n",
    "    \n",
    "    return x_copy[features]\n",
    "\n",
    "ImbalanceCalculator = FunctionTransformer(imbalance_calculator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " %%[markdown]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def cross_val_score(estimator, cv = tss, label = ''):\n",
    "    \n",
    "    X = train[~train.target.isna()]\n",
    "    y = X.pop('target')\n",
    "    \n",
    "    #initiate prediction arrays and score lists\n",
    "    val_predictions = np.zeros((len(X)))\n",
    "    #train_predictions = np.zeros((len(sample)))\n",
    "    train_scores, val_scores = [], []\n",
    "    \n",
    "    #training model, predicting prognosis probability, and evaluating metrics\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "        \n",
    "        model = clone(estimator)\n",
    "        \n",
    "        #define train set\n",
    "        X_train = X.iloc[train_idx]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        \n",
    "        #define validation set\n",
    "        X_val = X.iloc[val_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "        \n",
    "        #train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        #make predictions\n",
    "        train_preds = model.predict(X_train)\n",
    "        val_preds = model.predict(X_val)\n",
    "                  \n",
    "        val_predictions[val_idx] += val_preds\n",
    "        \n",
    "        #evaluate model for a fold\n",
    "        train_score = mean_absolute_error(y_train, train_preds)\n",
    "        val_score = mean_absolute_error(y_val, val_preds)\n",
    "        \n",
    "        #append model score for a fold to list\n",
    "        train_scores.append(train_score)\n",
    "        val_scores.append(val_score)\n",
    "    \n",
    "    print(f'Val Score: {np.mean(val_scores):.5f} ± {np.std(val_scores):.5f} | Train Score: {np.mean(train_scores):.5f} ± {np.std(train_scores):.5f} | {label}')\n",
    "    \n",
    "    return val_scores, val_predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "models = [\n",
    "    ('LightGBM', LGBMRegressor(random_state = seed, objective = 'mae', device_type = 'gpu'))\n",
    "]\n",
    "\n",
    "for (label, model) in models:\n",
    "    _ = cross_val_score(\n",
    "        make_pipeline(\n",
    "            ImbalanceCalculator,\n",
    "            model\n",
    "        ),\n",
    "        label = label\n",
    "    )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 12043\n",
      "[LightGBM] [Info] Number of data points in the train set: 476172, number of used features: 49\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 49 dense feature groups (23.61 MB) transferred to GPU in 0.018886 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.060201\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 12043\n",
      "[LightGBM] [Info] Number of data points in the train set: 952344, number of used features: 49\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 49 dense feature groups (47.23 MB) transferred to GPU in 0.033520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.050068\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 12043\n",
      "[LightGBM] [Info] Number of data points in the train set: 1428516, number of used features: 49\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 49 dense feature groups (70.84 MB) transferred to GPU in 0.053262 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.050068\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 12043\n",
      "[LightGBM] [Info] Number of data points in the train set: 1904688, number of used features: 49\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 49 dense feature groups (94.46 MB) transferred to GPU in 0.065033 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.060201\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 12043\n",
      "[LightGBM] [Info] Number of data points in the train set: 2380860, number of used features: 49\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 49 dense feature groups (118.07 MB) transferred to GPU in 0.081400 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.079870\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 12043\n",
      "[LightGBM] [Info] Number of data points in the train set: 2857032, number of used features: 49\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 49 dense feature groups (141.68 MB) transferred to GPU in 0.093010 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 12043\n",
      "[LightGBM] [Info] Number of data points in the train set: 3333204, number of used features: 49\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 49 dense feature groups (165.30 MB) transferred to GPU in 0.119312 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.060201\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 12043\n",
      "[LightGBM] [Info] Number of data points in the train set: 3809376, number of used features: 49\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 49 dense feature groups (188.91 MB) transferred to GPU in 0.125615 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.060201\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 12043\n",
      "[LightGBM] [Info] Number of data points in the train set: 4285548, number of used features: 49\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 49 dense feature groups (212.52 MB) transferred to GPU in 0.143149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.069737\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 12043\n",
      "[LightGBM] [Info] Number of data points in the train set: 4761720, number of used features: 49\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 2070, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 49 dense feature groups (236.14 MB) transferred to GPU in 0.150222 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] Start training from score -0.060201\n",
      "Val Score: 6.38752 ± 0.50903 | Train Score: 6.11303 ± 0.41645 | LightGBM\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}